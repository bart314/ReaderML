
\section{Rekenen met matrices}
Wanneer we willen rekenen met matrices, kunnen we twee gevallen onderscheiden: we kunnen werken met verschillen \textit{matrices}, of we kunnen werken met een matrix en een \textit{scalar}.

\subsection{Matrices met scalars}
Je kunt een matrix vermenigvuldigen met of delen door een scalaire waarde. Ook kun je een scalaire waarde bij een matrix optellen of er juist van aftrekken. Stel dat $\lambda$ een willekeurig getal is, en $A = (a_{ij})$ een willekeurige matrix, dan krijg je de \textit{scalaire vermenigvuldiging} van $A$ door elk element hiervan te vermenigvuldigen met $\lambda$: 

\[
3 \times \begin{bmatrix}
3 & 2 \\
4 & 8 \\
5 & 1 \\
\end{bmatrix} =
\begin{bmatrix}
3 \times 3 & 3 \times 2 \\
3 \times 4 & 3 \times 8 \\
3 \times 5 & 3 \times 1\\
\end{bmatrix} =
\begin{bmatrix}
9 & 6 \\
12 & 24 \\
15 & 3 \\
\end{bmatrix}
\]

Delen werkt feitelijk op een vergelijkbare manier; natuurlijk moet je er hier ook rekening mee houden dat je niet door 0 kunt delen. Ook het optellen en aftrekken van scalaire waarden gaat op een deze manier, al komt dat in de praktijk zelden voor. Verder gelden de standaard-eigenschappen: als je een matrix vermenigvuldigt met 0 krijg je (uiteraard) een matrix waarvan alle elementen 0 zijn en een matrix vermenigvuldigd met 1 is de matrix zelf. Een speciaal geval is nog de \textit{negatieve matrix} van $A = (a_{ij})$, die gedefinieerd is als $-1 \times A$, waarbij elk element $ij$ uit $-A$ gelijk is aan $-a_{ij}$. Hieruit volgt

\[
\begin{aligned}
A + (-A) &= 0\\
&= (-A) + A.
\end{aligned}
\]


\subsection{Twee of meer matrices}
Om twee of meer matrices te kunnen optellen of aftrekken, moeten ze \textit{compatibel} zijn, dat wil in dit geval zeggen dat ze dezelfde dimensionaliteit hebben; het resultaat is dan ook weer een matrix van deze grootte. Bij deze operaties gaan we uit van de individuele elementen van de matrices. Wanneer bijvoorbeeld $A = (a_{ij})$ en $B = (b_{ij})$ twee $m \times n$ matrices zijn, dan is hun som $C = (c_{ij}) = A + B$ een $m \times n$ matrix die gedefinieerd is als $c_{ij} = a_{ij} + b_{ij}$. Mutatis mutandis geldt hetzelfde voor aftrekken, zoals in de onderstaande voorbeelden:

\[
\begin{bmatrix}
1 & 0 \\
2 & 5 \\
3 & 1 \\
\end{bmatrix} +
\begin{bmatrix}
4 & 2 \\
2 & 4 \\
0 & 1 \\
\end{bmatrix} =
\begin{bmatrix}
5 & 2 \\
4 & 9 \\
3 & 2 \\
\end{bmatrix}
\]

\[
\begin{bmatrix}
5 & 2 \\
4 & 9 \\
3 & 2 \\
\end{bmatrix} -
\begin{bmatrix}
4 & 2 \\
2 & 4 \\
0 & 1 \\
\end{bmatrix} =
\begin{bmatrix}
1 & 0 \\
2 & 5 \\
3 & 1 \\
\end{bmatrix}
\]

De \textit{vermenigvuldiging} van matrices is iets complexer. Wanneer we bijvoorbeeld $A$ en $B$ met elkaar willen vermenigvuldigen, kan dat alleen wanneer het aantal \textit{kolommen} (in de voorbeelden hieronder aangegeven met $k$) van $A$ gelijk is aan het aantal \textit{rijen} (aangeduid met $r$) van $B$ (ook deze matrices moeten, kortom, \textit{compatibel} zijn, alleen betekent dat hier iets anders). Als $A=(a_{ij})$ een $m \times n$ matrix is en $B=(b_{jk})$ een $n \times p$ matrix, dan is hun product-matrix $C=AB=(c_{ik})$ een $m \times p$ matrix, waarvoor geldt:
%
\[
c_{ik} = \sum_{j=1}^n{a_{ij}b_{jk}}
\]
%
voor $i=1,2,\hdots,m$ en $k=1, 2, \hdots, p$. We vermenigvuldigen, kortom, elk element $ij$ uit $A$ met elk element $jk$ uit B en tellen die vermenigvuldigen bij elkaar op om element $ik$ uit $C$ te verkrijgen. In het voorbeeld hieronder vermenigvuldigen we een $2 \times 3$ matrix met een $3 \times 2$ matrix; het resultaat is dus een $2 \times 2$ matrix:


\[
\begin{aligned}
\begin{bmatrix}
1 & 3 & 2 \\
4 & 0 & 1 \\
\end{bmatrix} \times
\begin{bmatrix} 
1 & 3 \\
0 & 1 \\
5 & 2 \\
\end{bmatrix} &=
\begin{bmatrix}
r_1k_1 & r_1k_2 \\
r_2k_1 & r_2k_2 \\
\end{bmatrix} \\
&=
\begin{bmatrix}
1 \times 1 + 3 \times 0 + 2 \times 5 & 1 \times 3 + 3 \times 1 + 2 \times 2 \\
4 \times 1 + 0 \times 0 + 1 \times 5 & 4 \times 3 + 0 \times 1 + 1 \times 2 \\
\end{bmatrix} \\
&= 
\begin{bmatrix}
11 & 10 \\
9 & 14 \\
\end{bmatrix}
\end{aligned}
\]

Matrices hebben \textit{bijna} alle algebraïsche eigenschappen van gewone getallen. Het vermenigvuldigen van matrix met een (compatibele) identiteitsmatrix levert de matrix zelf op:
%
\[
I_mA = AI_m = A.
\]
%
Het vermenigvuldigen van een matrix met nul levert een nul-matrix op:
%
\[
A0 = 0.
\]
%
Vermenigvuldigingen zijn associatief en distributief, maar niet commutatief.
%
\[
\begin{aligned}
A(BC) &= (AB)C,\\
A(B+C) &= AB + AC, \\
AB &\neq BA.
\end{aligned}
\]
%

\subsection{Matrices en vectoren}
Omdat een vector feitelijk een $m \times 1$ matrix is (of een $1 \times m$ matrix als we het hebben over een rijvector), zijn de wiskundige operaties hierop bijna één op één over te nemen. Als bijvoorbeeld $A$ een $m \times n$ matrix is en $x$ een vector met lengte $n$, dan is $Ax$ een vector van lengte $m$:

\[
\begin{aligned}
\begin{bmatrix}
1 & 3 & 2\\
4 & 0 & 1\\
\end{bmatrix} \times
\begin{bmatrix}
1 \\
0 \\
5 \\
\end{bmatrix} &=
\begin{bmatrix}
r_1k_1 \\
r1k_2\\
\end{bmatrix} \\
&=
\begin{bmatrix}
1 \times  1 + 3 \times 0 + 2 \times 5\\
4 \times 1 + 0 \times 0 + 1 \times 5 \\
\end{bmatrix}\\
&=
\begin{bmatrix}
11\\
9\\
\end{bmatrix}
\end{aligned}
\]

Wanneer we werken met twee vectoren, krijgen we een interessante situatie. Stel dat $x$ en $y$ vectoren zijn met lengte $m$ en dat we deze twee met elkaar willen vermenigvuldigen. Omdat we twee matrices met dimensionaliteit $m \times n$ en $p \times n$ niet zonder meer met elkaar kunnen vermenigvuldigen (ze zijn immers niet compatibel), moeten we één van die twee transponeren (zodat we bijvoorbeeld $m \times n$ en $n \times p$ krijgen). Dit geldt natuurlijk ook voor vectoren, maar als we de linker vector $x$ transponeren, krijgen we feitelijk een $1 \times m$-dimensionale matrix. Als we die nu vermenigvuldigen met $y$ is het resultaat een matrix van 1 \times 1 – eigenlijk gewoon een \textit{getal} dus. Dit getal is het \textit{inproduct} van de vectoren $x$ en $y$:
 %
\[
\begin{aligned}
x^Ty &= \sum_{i=1}^m{x_iy_i}: \\
\begin{bmatrix}
1 & 2 & 3
\end{bmatrix} \times
\begin{bmatrix}
4\\5\\6\\
\end{bmatrix} &= 
\begin{bmatrix}
1 \times 4 + 2 \times 5 + 3 \times 6
\end{bmatrix}\\
&=32.
\end{aligned}
\]
%
De superscript T (in $x^T$) wordt soms ook weggelaten, als uit de context duidelijk is dat het om een inproduct gaat. Ook zie je wel het gebruik van de vermenigvuldigingspunt om dit nog explicieter aan te geven ($x \cdot y$). 

Omgekeerd krijgen we als we de rechter vector $y$ transponeren een $m \times m$ matrix die bekend staat als het \textit{tensorproduct} (niet te verwarren met het \textit{uitproduct}). Stel dat $Z$ de matrix is die verkregen wordt door $xy^T$, dan geldt $z_{ij}=x_iy_j$. 

